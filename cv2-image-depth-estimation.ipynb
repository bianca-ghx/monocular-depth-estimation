{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f87d7129-cfd3-4b2b-b115-9b6035190447",
    "_uuid": "2f3b857c-326d-4d4c-91a4-f7580ebc030a",
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:15.613199Z",
     "iopub.status.busy": "2023-05-17T18:31:15.612685Z",
     "iopub.status.idle": "2023-05-17T18:31:25.756822Z",
     "shell.execute_reply": "2023-05-17T18:31:25.755540Z",
     "shell.execute_reply.started": "2023-05-17T18:31:15.613154Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "# Import libraries and modules required for the implementation of the depth estimation model\n",
    "from zipfile import ZipFile\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, UpSampling2D, LeakyReLU, Concatenate\n",
    "from keras import Model\n",
    "from keras.applications import DenseNet169, DenseNet121\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e97c2637-b643-4266-ab19-2081ec723cbe",
    "_uuid": "64a9cb92-6615-4eef-b04d-700be15a0fcf"
   },
   "source": [
    "# **Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df922f33-8022-491a-8439-271ac8d3477d",
    "_uuid": "15561661-1449-4843-a849-91a80b4bff79",
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.761432Z",
     "iopub.status.busy": "2023-05-17T18:31:25.760515Z",
     "iopub.status.idle": "2023-05-17T18:31:25.778895Z",
     "shell.execute_reply": "2023-05-17T18:31:25.777683Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.761399Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    # Load and preprocess the training data from a CSV file\n",
    "    def __init__(self, csv_file='/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_train.csv'):\n",
    "        self.shape_rgb = (480, 640, 3)\n",
    "        self.shape_depth = (240, 320, 1)\n",
    "        self.read_data(csv_file)\n",
    "\n",
    "    # Resize the image\n",
    "    def resize_img(self, img, resolution=480):\n",
    "        return resize(img, (resolution, int(resolution * 4 / 3)), preserve_range=True, mode='reflect', anti_aliasing=True)\n",
    "\n",
    "    # Read data from the CSV file\n",
    "    def read_data(self, csv_file):\n",
    "        data = [row.split(',') for row in open(csv_file, 'r').read().split('\\n') if len(row) > 0]\n",
    "        data = shuffle(data, random_state=0)\n",
    "        self.filenames = [f\"/kaggle/input/nyu-depth-v2/nyu_data/{i[0]}\" for i in data]\n",
    "        self.labels = [f\"/kaggle/input/nyu-depth-v2/nyu_data/{i[1]}\" for i in data]\n",
    "        self.length = len(self.filenames)\n",
    "\n",
    "    # Preprocess the data by decoding the images and converting them to float32\n",
    "    def parse_function(self, filename, label):\n",
    "        img = tf.image.decode_jpeg(tf.io.read_file(filename))\n",
    "        depth = tf.image.decode_jpeg(tf.io.read_file(label))\n",
    "        rgb = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
    "        depth = tf.image.convert_image_dtype(tf.image.resize(depth, [self.shape_depth[0], self.shape_depth[1]]) / 255.0, dtype=tf.float32)\n",
    "        depth = 1000 / tf.clip_by_value(depth * 1000, 10, 1000)\n",
    "        return rgb, depth\n",
    "\n",
    "    # Batch the dataset\n",
    "    def get_batched_dataset(self, batch_size):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.filenames, self.labels))\n",
    "        dataset = dataset.shuffle(buffer_size=self.length, reshuffle_each_iteration=True)\n",
    "        dataset = dataset.repeat()\n",
    "        dataset = dataset.map(map_func=self.parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        dataset = dataset.batch(batch_size=batch_size)\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec80dad3-38d7-47b9-aef9-54b5b6593386",
    "_uuid": "3d58e1ca-ce88-40b8-9bd2-108a6a5b344f"
   },
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.780856Z",
     "iopub.status.busy": "2023-05-17T18:31:25.780415Z",
     "iopub.status.idle": "2023-05-17T18:31:25.792700Z",
     "shell.execute_reply": "2023-05-17T18:31:25.790928Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.780818Z"
    }
   },
   "outputs": [],
   "source": [
    "class UpscaleBlock(Model):\n",
    "    # Define an upscaling block with skip connections\n",
    "    def __init__(self, filters, name):\n",
    "        super(UpscaleBlock, self).__init__()\n",
    "        self.up = UpSampling2D(size=(2, 2), interpolation='bilinear', name=name + '_upsampling2d')\n",
    "        self.concat = Concatenate(name=name + '_concat')\n",
    "        self.convA = Conv2D(filters, 3, 1, 'same', name=name + '_convA')\n",
    "        self.reluA = LeakyReLU(alpha=0.2)\n",
    "        self.convB = Conv2D(filters, 3, 1, 'same', name=name + '_convB')\n",
    "        self.reluB = LeakyReLU(alpha=0.2)\n",
    "\n",
    "    # Define the forward pass through the block\n",
    "   \n",
    "    def call(self, x):\n",
    "        upsampled = self.up(x[0]) # Upsample the input tensor\n",
    "        concatenated = self.concat([upsampled, x[1]]) # Concatenate the upsampled tensor with the skip connection\n",
    "        convA_output = self.reluA(self.convA(concatenated)) # Perform the convolution and apply the LeakyReLU activation function\n",
    "        convB_output = self.reluB(self.convB(convA_output)) # Perform another convolution and apply the LeakyReLU activation function\n",
    "        return convB_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.796704Z",
     "iopub.status.busy": "2023-05-17T18:31:25.796170Z",
     "iopub.status.idle": "2023-05-17T18:31:25.806342Z",
     "shell.execute_reply": "2023-05-17T18:31:25.805275Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.796524Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    # Define an encoder based on the DenseNet-169 architecture\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.base_model = DenseNet169(input_shape=(None, None, 3), include_top=False, weights='imagenet')\n",
    "        print(f'Base model loaded {DenseNet169.__name__}')\n",
    "        \n",
    "        layer_names = ['pool1', 'pool2_pool', 'pool3_pool', 'conv1/relu']\n",
    "        outputs = [self.base_model.get_layer(name).output for name in layer_names]\n",
    "        outputs.insert(0, self.base_model.outputs[-1])\n",
    "        self.encoder = Model(inputs=self.base_model.inputs, outputs=outputs)\n",
    "\n",
    "    # Define the forward pass through the encoder\n",
    "    def call(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.808563Z",
     "iopub.status.busy": "2023-05-17T18:31:25.808147Z",
     "iopub.status.idle": "2023-05-17T18:31:25.820512Z",
     "shell.execute_reply": "2023-05-17T18:31:25.819003Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.808525Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    # Define a decoder with skip connections and upscaling blocks\n",
    "    def __init__(self, decode_filters):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv2 = Conv2D(decode_filters, 1, padding='same', name='conv2')\n",
    "        self.up1 = UpscaleBlock(decode_filters // 2, name='up1')\n",
    "        self.up2 = UpscaleBlock(decode_filters // 4, name='up2')\n",
    "        self.up3 = UpscaleBlock(decode_filters // 8, name='up3')\n",
    "        self.up4 = UpscaleBlock(decode_filters // 16, name='up4')\n",
    "        self.conv3 = Conv2D(1, 3, 1, padding='same', name='conv3')\n",
    "\n",
    "    # Define the forward pass through the decoder\n",
    "    def call(self, features):\n",
    "        x, pool1, pool2, pool3, conv1 = features\n",
    "        up0 = self.conv2(x) # Perform a convolution on the input tensor\n",
    "        up1 = self.up1([up0, pool3]) # Apply an upscaling block with skip connections\n",
    "        up2 = self.up2([up1, pool2]) # Apply another upscaling block with skip connections\n",
    "        up3 = self.up3([up2, pool1]) # Apply another upscaling block with skip connections\n",
    "        up4 = self.up4([up3, conv1]) # Apply another upscaling block with skip connections\n",
    "        return self.conv3(up4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.822131Z",
     "iopub.status.busy": "2023-05-17T18:31:25.821796Z",
     "iopub.status.idle": "2023-05-17T18:31:25.834345Z",
     "shell.execute_reply": "2023-05-17T18:31:25.833367Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.822101Z"
    }
   },
   "outputs": [],
   "source": [
    "class DepthEstimate(Model):\n",
    "    # Define the full depth estimation model\n",
    "    def __init__(self):\n",
    "        super(DepthEstimate, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(decode_filters=int(self.encoder.layers[-1].output[0].shape[-1] // 2))\n",
    "        print('\\nModel created.')\n",
    "\n",
    "    # Define the forward pass through the depth estimation model\n",
    "    def call(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.836918Z",
     "iopub.status.busy": "2023-05-17T18:31:25.836182Z",
     "iopub.status.idle": "2023-05-17T18:31:25.846984Z",
     "shell.execute_reply": "2023-05-17T18:31:25.845873Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.836877Z"
    }
   },
   "outputs": [],
   "source": [
    "def depth_loss_function(y_true, y_pred, theta=0.1, maxDepthVal=100.0):\n",
    "    # Define the depth loss function\n",
    "    def mean_abs_diff(a, b):\n",
    "        return K.mean(K.abs(a - b), axis=-1)\n",
    "    def image_gradients(img):\n",
    "        return tf.image.image_gradients(img)\n",
    "\n",
    "    l_depth = mean_abs_diff(y_pred, y_true) # Compute the mean absolute difference between the predicted and ground truth depth maps\n",
    "    dy_true, dx_true = image_gradients(y_true) # Compute the image gradients of the ground truth depth map\n",
    "    dy_pred, dx_pred = image_gradients(y_pred) # Compute the image gradients of the predicted depth map\n",
    "    l_edges = mean_abs_diff(dy_pred, dy_true) + mean_abs_diff(dx_pred, dx_true) # Compute the mean absolute difference between the gradients of the predicted and ground truth depth maps\n",
    "\n",
    "    l_ssim = K.clip((1 - tf.image.ssim(y_true, y_pred, maxDepthVal)) * 0.5, 0, 1) # Compute the structural similarity index (SSIM) between the predicted and ground truth depth maps\n",
    "\n",
    "    # Compute the final loss by combining the mean absolute differences and the SSIM\n",
    "    return l_ssim + K.mean(l_edges) + theta * K.mean(l_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.849274Z",
     "iopub.status.busy": "2023-05-17T18:31:25.848574Z",
     "iopub.status.idle": "2023-05-17T18:31:25.860349Z",
     "shell.execute_reply": "2023-05-17T18:31:25.859404Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.849236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "def load_test_data():\n",
    "    # Load the test data from a ZIP archive\n",
    "    print('Loading test data...', end='')\n",
    "\n",
    "    def extract_zip(input_zip):\n",
    "        with ZipFile(input_zip) as zf:\n",
    "            return {name: zf.read(name) for name in zf.namelist()}\n",
    "\n",
    "    data = extract_zip('nyu_test.zip')\n",
    "\n",
    "    def load_data_from_zip(key):\n",
    "        return np.load(BytesIO(data[key]))\n",
    "\n",
    "    rgb = load_data_from_zip('eigen_test_rgb.npy')\n",
    "    depth = load_data_from_zip('eigen_test_depth.npy')\n",
    "    crop = load_data_from_zip('eigen_test_crop.npy')\n",
    "\n",
    "    print('Test data loaded.\\n')\n",
    "\n",
    "    return rgb, depth, crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.862593Z",
     "iopub.status.busy": "2023-05-17T18:31:25.861869Z",
     "iopub.status.idle": "2023-05-17T18:31:25.870203Z",
     "shell.execute_reply": "2023-05-17T18:31:25.869216Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.862552Z"
    }
   },
   "outputs": [],
   "source": [
    "def DepthNorm(x, maxDepth):\n",
    "    return maxDepth / x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.875018Z",
     "iopub.status.busy": "2023-05-17T18:31:25.874335Z",
     "iopub.status.idle": "2023-05-17T18:31:25.882525Z",
     "shell.execute_reply": "2023-05-17T18:31:25.881723Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.874961Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, images, minDepth=10, maxDepth=1000, batch_size=2):\n",
    "    # Use the trained depth estimation model to make predictions on input images\n",
    "    def preprocess_images(images):\n",
    "        if images.ndim == 2:\n",
    "            images = np.stack((images, images, images), axis=2)\n",
    "        if images.ndim == 3:\n",
    "            images = images[np.newaxis, ...]\n",
    "        return images\n",
    "\n",
    "    images = preprocess_images(images)\n",
    "    predictions = model.predict(images, batch_size=batch_size)\n",
    "\n",
    "    return np.clip(DepthNorm(predictions, maxDepth=1000), minDepth, maxDepth) / maxDepth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.884683Z",
     "iopub.status.busy": "2023-05-17T18:31:25.883933Z",
     "iopub.status.idle": "2023-05-17T18:31:25.895177Z",
     "shell.execute_reply": "2023-05-17T18:31:25.894347Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.884646Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_up(scale, images):\n",
    "    # Scale up the images\n",
    "    def resize_image(img, scale):\n",
    "        output_shape = (scale * img.shape[0], scale * img.shape[1])\n",
    "        return resize(img, output_shape, order=1, preserve_range=True, mode='reflect', anti_aliasing=True)\n",
    "\n",
    "    return np.array([resize_image(img, scale) for img in images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.897490Z",
     "iopub.status.busy": "2023-05-17T18:31:25.896803Z",
     "iopub.status.idle": "2023-05-17T18:31:25.914046Z",
     "shell.execute_reply": "2023-05-17T18:31:25.912917Z",
     "shell.execute_reply.started": "2023-05-17T18:31:25.897450Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, rgb, depth, crop, batch_size=6):\n",
    "    # Evaluate the performance of the depth estimation model\n",
    "    def compute_errors(gt, pred):\n",
    "        thresh = np.maximum((gt / pred), (pred / gt))\n",
    "        a1, a2, a3 = [(thresh < 1.25 ** i).mean() for i in range(1, 4)]\n",
    "        abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "        rmse = np.sqrt(np.mean((gt - pred) ** 2))\n",
    "        log_10 = np.mean(np.abs(np.log10(gt) - np.log10(pred)))\n",
    "        return a1, a2, a3, abs_rel, rm\n",
    "    depth_scores = np.zeros((6, len(rgb)))\n",
    "\n",
    "    for i in range(0, len(rgb), batch_size):\n",
    "        x = rgb[i:i + batch_size]\n",
    "        true_y = depth[i:i + batch_size]\n",
    "        pred_y = 10 * scale_up(2, predict(model, x / 255)[:, :, :, 0])\n",
    "        pred_y_flip = 10 * scale_up(2, predict(model, x[..., ::-1, :] / 255)[:, :, :, 0])\n",
    "\n",
    "        true_y = true_y[:, crop[0]:crop[1] + 1, crop[2]:crop[3] + 1]\n",
    "        pred_y = pred_y[:, crop[0]:crop[1] + 1, crop[2]:crop[3] + 1]\n",
    "        pred_y_flip = pred_y_flip[:, crop[0]:crop[1] + 1, crop[2]:crop[3] + 1]\n",
    "\n",
    "        for j, (gt, p, pf) in enumerate(zip(true_y, pred_y, pred_y_flip)):\n",
    "            depth_scores[:, i + j] = compute_errors(gt, 0.5 * (p + np.fliplr(pf)))\n",
    "\n",
    "    mean_errors = depth_scores.mean(axis=1)\n",
    "\n",
    "    print(\"{:>10}, {:>10}, {:>10}, {:>10}, {:>10}, {:>10}\".format('a1', 'a2', 'a3', 'rel', 'rms', 'log_10'))\n",
    "    print(\"{:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}, {:10.4f}\".format(*mean_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-17T18:31:25.917971Z",
     "iopub.status.busy": "2023-05-17T18:31:25.917100Z"
    }
   },
   "outputs": [],
   "source": [
    "# set fitting parameters\n",
    "batch_size     = 8\n",
    "learning_rate  = 0.0001\n",
    "epochs         = 10\n",
    "\n",
    "# model creation\n",
    "model = DepthEstimate()\n",
    "\n",
    "dl = DataLoader()\n",
    "train_generator = dl.get_batched_dataset(batch_size)\n",
    "\n",
    "print('Data loader ready.')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate, amsgrad=True)\n",
    "\n",
    "# model compilation using custom loss function and the Adam optimizer\n",
    "model.compile(loss=depth_loss_function, optimizer=optimizer)\n",
    "\n",
    "# checkpoint saving, used for evaluation\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1)\n",
    "\n",
    "# model fitting\n",
    "model.fit(train_generator, epochs=5, steps_per_epoch=dl.length//batch_size, callbacks=[cp_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
